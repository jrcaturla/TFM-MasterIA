{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Instalación de librerías"
      ],
      "metadata": {
        "id": "zp_Vftp6F1Et"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jytc2KOX9oOu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcc4690d-7698-4f68-dbce-b4597d317349",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.5.15)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.0)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, portalocker, dill, colorama, tiktoken, sacrebleu, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed colorama-0.4.6 datasets-3.0.0 dill-0.3.8 multiprocess-0.70.16 portalocker-2.10.1 pyarrow-17.0.0 sacrebleu-2.4.3 tiktoken-0.7.0 xxhash-3.5.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.5/373.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pandas  sacrebleu tiktoken datasets\n",
        "!pip install --upgrade openai --quiet\n",
        "\n",
        "import openai\n",
        "import pandas as pd\n",
        "from sacrebleu import corpus_bleu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "import os\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "GNdGpXKpGKgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga de archivos para enviar en la consulta, dataset de enternamiento y el dataset de prueba sin las traducciones : train_dataset.txt y test_souces.txt"
      ],
      "metadata": {
        "id": "sQTb-bT7LZQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "W55qEjhdRrun",
        "outputId": "36c6dfa7-dc5c-4b54-db6d-e9445faf8859",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9db02772-60e4-4dda-bd19-07312cc27f03\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9db02772-60e4-4dda-bd19-07312cc27f03\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test_dataset (1).txt to test_dataset (1).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path= '/content/train_dataset.txt'\n",
        "test_path = '/content/test_sources.txt'\n",
        "test_completed_path = '/content/test_dataset.txt'"
      ],
      "metadata": {
        "id": "TNihwjSrSCSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"gpt-4o\"\n",
        "# Definir el tokenizador para el modelo GPT-4\n",
        "tokenizer = tiktoken.encoding_for_model(MODEL)\n",
        "\n",
        "def count_tokens(text):\n",
        "    \"\"\"Función para contar tokens en un texto usando tiktoken.\"\"\"\n",
        "    tokens = tokenizer.encode(text)\n",
        "    return len(tokens)\n",
        "\n",
        "def read_file(file_path):\n",
        "    \"\"\"Función para leer el contenido de un archivo.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n"
      ],
      "metadata": {
        "id": "GkXmJ757V3Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Conteo de tokens fuera de la llamada al modelo\n",
        "\n",
        "# Leer el archivo de prueba\n",
        "test_content = read_file(test_path)\n",
        "\n",
        "# Contar tokens en el contenido del archivo\n",
        "test_tokens = count_tokens(test_content)\n",
        "print(f\"Tokens en el archivo de prueba: {test_tokens}\")\n",
        "\n",
        "## Leer el archivo de test con traducciones\n",
        "test_complet_content = read_file(test_completed_path)\n",
        "\n",
        "# Contar tokens en el contenido del archivo\n",
        "test_complet_tokens = count_tokens(test_complet_content)\n",
        "print(f\"Tokens en el archivo de prueba con traducciones: {test_complet_tokens }\")\n",
        "\n",
        "# Leer el archivo de entrenamiento\n",
        "train_content = read_file(train_path)\n",
        "\n",
        "# Contar tokens en el archivo de entrenamiento\n",
        "train_tokens = count_tokens(train_content)\n",
        "print(f\"Tokens en el archivo de entrenamiento: {train_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK61m97SXA1l",
        "outputId": "bae20bee-0ae1-468d-cecd-884814c386fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens en el archivo de prueba: 3091\n",
            "Tokens en el archivo de prueba con traducciones: 5233\n",
            "Tokens en el archivo de entrenamiento: 43298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(file_path):\n",
        "    \"\"\"Función para leer el contenido del archivo.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def translate_and_count_tokens(model, input_text):\n",
        "    \"\"\"Función para enviar texto al modelo y contar tokens usados.\"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": input_text}],\n",
        "            temperature=0,\n",
        "            stream=True,\n",
        "            stream_options={\"include_usage\": True}\n",
        "        )\n",
        "\n",
        "        # Procesa la salida y cuenta los tokens\n",
        "        content = \"\"\n",
        "        for chunk in response:\n",
        "            if chunk.choices:\n",
        "                content += chunk.choices[0].delta.content\n",
        "\n",
        "        # Si el stream se cerró correctamente, los últimos datos incluirán la información de uso\n",
        "        if 'usage' in chunk:\n",
        "            print(f\"Token usage: {chunk.usage}\")\n",
        "        else:\n",
        "            print(\"No usage info available\")\n",
        "\n",
        "        return content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "s-kCfvVE3_NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traducciones sin envío previo del dataset de entrenamiento"
      ],
      "metadata": {
        "id": "KjXjTryHGiUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "api_key = 'Clave_OpenAI'\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "MODEL = \"gpt-4o\"\n",
        "\n",
        "def read_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def translate_text(test):\n",
        "    with open(\"completed_test_sources.txt\", \"w\") as output_file:\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant who learns languages that you have just been entering\"},\n",
        "                    {\"role\": \"user\", \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": \"please complete with translations in Spanish\"},\n",
        "                        {\"type\": \"text\", \"text\": test}\n",
        "                    ]}\n",
        "                ],\n",
        "                temperature=0.5,\n",
        "            )\n",
        "\n",
        "            # Verifica y guarda la respuesta\n",
        "            if response.choices:\n",
        "                extracted_text = response.choices[0].message.content\n",
        "                output_file.write(f\"{extracted_text}\\n\\n\")\n",
        "            else:\n",
        "                output_file.write(\"No response received.\\n\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            output_file.write(f\"An error occurred while processing: {e}\\n\\n\")\n",
        "\n",
        "# Leer el contenido de los archivos de entrenamiento y test\n",
        "test_content = read_file(\"test_sources.txt\")\n",
        "\n",
        "# Llama a la función para realizar la traducción\n",
        "translate_text(test_content)"
      ],
      "metadata": {
        "id": "eMFV13ZhhP_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traducciones enviando previamente el dataste de entrenamiento"
      ],
      "metadata": {
        "id": "pUpW0CENG1mR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "api_key = 'Clave_OpenAI'\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "MODEL = \"gpt-4o\"\n",
        "\n",
        "def read_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def translate_text(examples, test):\n",
        "    with open(\"completed_test_sources_with_train.txt\", \"w\") as output_file:\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant who learns languages that you have just been entering\"},\n",
        "                    {\"role\": \"user\", \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": \"please complete with translations\"},\n",
        "                        {\"type\": \"text\", \"text\": examples},\n",
        "                        {\"type\": \"text\", \"text\": test}\n",
        "                    ]}\n",
        "                ],\n",
        "                temperature=0.5,\n",
        "            )\n",
        "\n",
        "            # Verifica y guarda la respuesta\n",
        "            if response.choices:\n",
        "                extracted_text = response.choices[0].message.content\n",
        "                output_file.write(f\"{extracted_text}\\n\\n\")\n",
        "            else:\n",
        "                output_file.write(\"No response received.\\n\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            output_file.write(f\"An error occurred while processing: {e}\\n\\n\")\n",
        "\n",
        "# Lee el contenido de los archivos de entrenamiento y test\n",
        "train_content = read_file(\"train_dataset.txt\")\n",
        "test_content = read_file(\"test_sources.txt\")\n",
        "\n",
        "# Llama a la función para realizar la traducción\n",
        "translate_text(train_content, test_content)\n"
      ],
      "metadata": {
        "id": "ipUQ4nNaGoyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluación de las traducciones"
      ],
      "metadata": {
        "id": "evQv-8cTHHJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sacrebleu\n",
        "\n",
        "def load_translations(file_path):\n",
        "    translations = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            if line.startswith(\"target:\"):\n",
        "                # Extrae el texto después de \"target:\" y quita espacios y comillas\n",
        "                translation = line.split(\"target:\", 1)[1].strip().strip(\"'\")\n",
        "                translations.append(translation)\n",
        "    return translations\n",
        "\n",
        "def clean_translations(translations):\n",
        "    return [t.strip(\"'\") for t in translations]\n",
        "\n",
        "# Cargar las traducciones del modelo y las referencias\n",
        "model_translations = load_translations(\"completed_test_sources.txt\")\n",
        "model_translations_with_train = load_translations(\"completed_test_sources_with_train.txt\")\n",
        "reference_translations = [load_translations(\"test_dataset.txt\")]  # Las referencias deben ser una lista de listas para sacrebleu\n",
        "# Calcular el score SacreBLEU\n",
        "bleu_score = sacrebleu.corpus_bleu(model_translations, reference_translations)\n",
        "print(f\"SacreBLEU score: {bleu_score.score}\")\n",
        "bleu_score = sacrebleu.corpus_bleu(model_translations_with_train, reference_translations)\n",
        "print(f\"SacreBLEU with train score: {bleu_score.score}\")\n",
        "'''\n",
        "\n",
        "\n",
        "Con temperature 0:\n",
        "SacreBLEU score: 25.14531263132354\n",
        "SacreBLEU with train score: 29.57642290956401\n",
        "\n",
        "\n",
        "Con temperature 0.5:\n",
        "SacreBLEU score: 26.03962010287146\n",
        "SacreBLEU with train score: 31.029365354664073\n",
        "\n",
        "temperature 0.7\n",
        "SacreBLEU score: 24.97737058250025\n",
        "SacreBLEU with train score: 29.252665972066076\n",
        "\n",
        "con temperature 0.8\n",
        "SacreBLEU score: 26.28803002322718\n",
        "SacreBLEU with train score: 28.65448865098154\n",
        "\n",
        "\n",
        "temperature 1.0:\n",
        "SacreBLEU score: 24.086772498679917\n",
        "SacreBLEU with train score: 25.02056505769791\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "JEH4MjfGRMlF",
        "outputId": "2660633e-756a-4b07-fe8f-615725c3ba82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SacreBLEU score: 27.044730862263105\n",
            "SacreBLEU with train score: 31.237860248126175\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nCon temperature 0.5:\\ncon las comillas .. 28.99120053309069\\nSacreBLEU score: 26.03962010287146\\nSacreBLEU with train score: 31.029365354664073\\n\\nCon temperature 0:\\nSacreBLEU score: 25.14531263132354\\nSacreBLEU with train score: 29.57642290956401\\n\\ncon temperature 0.8 pero ejecutando antes con train:\\nSacreBLEU score: 25.749454617412212\\nSacreBLEU with train score: 23.15523487829909\\n\\ncon temperature 0.8\\nSacreBLEU score: 26.28803002322718\\nSacreBLEU with train score: 28.65448865098154\\n\\nSacreBLEU score: 22.56662950496349\\nSacreBLEU with train score: 27.00434561808753\\n\\n\\ntemperature 1.0:\\nSacreBLEU score: 24.086772498679917\\nSacreBLEU with train score: 25.02056505769791\\n\\ntemperature 0.5 8:50\\n\\nSacreBLEU score: 23.383724229098057\\nSacreBLEU with train score: 30.061177459903313\\n\\ntemperature 0.7 \\nSacreBLEU score: 24.97737058250025\\nSacreBLEU with train score: 29.252665972066076\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}